import os
import json
import pickle
import pandas as pd
import csv
import re, unicodedata as ud
from pathlib import Path
from textgrid import TextGrid, IntervalTier
from multiprocessing import Pool
from tqdm import tqdm

# JSON ë° TextGrid íŒŒì¼ ëª©ë¡ ì €ì¥ìš© pkl íŒŒì¼
json_pkl_path = "json_file_list.pkl"
tg_pkl_path = "tg_file_list.pkl"
tsv_original = Path("data/dialect_output.tsv")
# JSON ë° TextGrid íŒŒì¼ ê²½ë¡œ ì„¤ì •
json_root = "/home/yugwon/nas/audio/ASR/dialect1/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°"
textgrid_root = "/home/yugwon/mfa/tg_dir"

# í—ˆìš© ì˜¤ì°¨ (tolerance) ì„¤ì • (ms ë‹¨ìœ„)
tolerance_levels = [10, 25, 50, 100]

_HANGUL_RE = re.compile(r'[ê°€-í£]+')
def _clean(txt: str) -> str:
    """NFC ì •ê·œí™” ë’¤ ì™„ì„±í˜• í•œê¸€ ìŒì ˆë§Œ ë‚¨ê¸´ë‹¤"""
    return ''.join(_HANGUL_RE.findall(ud.normalize('NFC', txt)))

# === JSON íŒŒì¼ ëª©ë¡ ê²€ìƒ‰ ë° ì €ì¥ ===
def get_json_files():
    json_files = {}

    if os.path.exists(json_pkl_path):
        print(f"ğŸ“‚ ê¸°ì¡´ JSON íŒŒì¼ ëª©ë¡ ë¡œë“œ ì¤‘: {json_pkl_path}")
        with open(json_pkl_path, "rb") as f:
            json_list = pickle.load(f)  # ê¸°ì¡´ pkl íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ë¨

        # ë¦¬ìŠ¤íŠ¸ë¥¼ {basename: path} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        for json_path in json_list:
            basename = os.path.splitext(os.path.basename(json_path))[0]  # í™•ì¥ì ì œê±°í•˜ì—¬ basename ì¶”ì¶œ
            json_files[basename] = json_path

        return json_files  # ë³€í™˜ëœ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

    # ê¸°ì¡´ pkl íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ê²€ìƒ‰
    json_list = []
    for root, _, files in os.walk(json_root):
        if "ì¶©ì²­ë„" in root:  # 'ì¶©ì²­ë„'ê°€ í¬í•¨ëœ ê²½ë¡œë§Œ í•„í„°ë§
            for file in files:
                if file.endswith(".json"):
                    json_path = os.path.join(root, file)
                    json_list.append(json_path)
                    basename = os.path.splitext(file)[0]
                    json_files[basename] = json_path

    # JSON íŒŒì¼ ëª©ë¡ì„ pickleë¡œ ì €ì¥
    with open(json_pkl_path, "wb") as f:
        pickle.dump(json_list, f)  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥

    print(f"âœ… JSON íŒŒì¼ ëª©ë¡ ì €ì¥ ì™„ë£Œ: {json_pkl_path} ({len(json_files)}ê°œ íŒŒì¼)")
    return json_files


# === TextGrid íŒŒì¼ ëª©ë¡ ê²€ìƒ‰ ë° ì €ì¥ ===
def get_textgrid_files():
    if os.path.exists(tg_pkl_path):
        print(f"ğŸ“‚ ê¸°ì¡´ TextGrid íŒŒì¼ ëª©ë¡ ë¡œë“œ ì¤‘: {tg_pkl_path}")
        with open(tg_pkl_path, "rb") as f:
            return pickle.load(f)

    textgrid_files = []
    for root, _, files in os.walk(textgrid_root):
        for file in files:
            if file.endswith(".TextGrid"):
                textgrid_files.append(os.path.join(root, file))

    # TextGrid íŒŒì¼ ëª©ë¡ì„ pickleë¡œ ì €ì¥
    with open(tg_pkl_path, "wb") as f:
        pickle.dump(textgrid_files, f)

    print(f"âœ… TextGrid íŒŒì¼ ëª©ë¡ ì €ì¥ ì™„ë£Œ: {tg_pkl_path} ({len(textgrid_files)}ê°œ íŒŒì¼)")
    return textgrid_files

# JSON íŒŒì¼ì—ì„œ ì–´ì ˆë³„ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì¶œ
def parse_json(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {file_path}")
        return None

    word_segments = []
    for seg in data.get("transcription", {}).get("segments", []):
        try:
            start_time = float(seg["startTime"].split(":")[-1])  # ì´ˆ ë‹¨ìœ„ ë³€í™˜
            end_time = float(seg["endTime"].split(":")[-1])
            word = seg["dialect"]  # ì–´ì ˆ í…ìŠ¤íŠ¸
            word_segments.append((start_time, end_time, word))
        except (KeyError, ValueError):
            print(f"âš ï¸ JSON ë°ì´í„° ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {file_path}")

    return word_segments if word_segments else None

def load_original_words(tsv_path: Path) -> dict[str, list[str]]:
    """
    TSV(íƒ­ êµ¬ë¶„) â†’ {wav_id: [word1, word2, ...]} ë”•ì…”ë„ˆë¦¬
    í•„ìš”í•œ í—¤ë”: 'wav_filepath', 'text'
    """
    mapping: dict[str, list[str]] = {}

    with tsv_path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        if not {"wav_filepath", "text"} <= set(reader.fieldnames or []):
            raise ValueError("TSV í—¤ë”ì— 'wav_filepath' ë˜ëŠ” 'text' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")

        for idx, row in enumerate(reader, start=2):   # 2 = ì²« ë°ì´í„° í–‰
            wav_id = (row.get("wav_filepath") or "").strip()
            sentence = (row.get("text") or "").strip()
            if not wav_id or not sentence:
                print(f"[WARN] {idx}í–‰: ë¹ˆ í•„ë“œ â†’ ê±´ë„ˆëœ€")
                continue
            mapping[wav_id] = sentence.split()        # ê³µë°± ê¸°ì¤€ ë¶„í• 

    print(f"âœ… ì›ë¬¸ ì–´ì ˆ ë§¤í•‘ ë¡œë“œ: {len(mapping):,}ê°œ")
    return mapping

# TextGrid íŒŒì¼ì—ì„œ ì–´ì ˆë³„ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì¶œ
def parse_textgrid(file_path: str):
    """
    TextGrid 'word(s)' tierì˜ interval ì¡°ê°ì„ ì´ì–´ë¶™ì—¬
    TSV ì›ë¬¸ ì–´ì ˆê³¼ ë§¤ì¹­â€†â†’â€†(start, end, word) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    * í•œê¸€ ìŒì ˆ(ê°€-í£)ë§Œ ë¹„êµ, ë¬¸ì¥ë¶€í˜¸Â·ì˜ì–´Â·ìëª¨ ë¶„ë¦¬ ì œê±°
    """
    basename = Path(file_path).stem
    target_words = [_clean(w) for w in orig_words.get(basename, []) if _clean(w)]
    if not target_words:
        return None

    try:
        tg = TextGrid.fromFile(file_path)
    except Exception as e:
        print(f"âŒ TG ì½ê¸° ì˜¤ë¥˜: {file_path} â€“ {e}")
        return None

    # words tier ì°¾ê¸°
    word_tier = next(
        (t for t in tg.tiers if isinstance(t, IntervalTier)
         and t.name.lower().startswith("word")),
        None
    )
    if word_tier is None:
        return None

    results, cur_word, start_t, tidx = [], "", None, 0

    for iv in word_tier.intervals:
        piece = _clean(iv.mark)
        if not piece:           # ë¹ˆ ë¼ë²¨ â†’ ê±´ë„ˆëœ€
            continue

        if cur_word == "":
            start_t = iv.minTime
        cur_word += piece

        if tidx >= len(target_words):   # ì›ë¬¸ ì–´ì ˆ ì´ˆê³¼ â†’ ì¤‘ë‹¨
            break
        target = target_words[tidx]

        if cur_word == target:          # ì •í™•íˆ ë§¤ì¹­
            results.append((start_t, iv.maxTime, target))
            cur_word, start_t = "", None
            tidx += 1
        elif len(cur_word) > len(target) or not target.startswith(cur_word):
            # ë¶ˆì¼ì¹˜: í˜„ì¬ intervalë¶€í„° ë‹¤ì‹œ ì‹œì‘
            print(f"[WARN] ë¶ˆì¼ì¹˜: {basename} | '{cur_word}' vs '{target}'")
            cur_word, start_t = "", None  # ë²„í¼Â·ì‹œê° ë¦¬ì…‹

    return results if results else None

# JSONê³¼ TextGrid ë¹„êµ í›„ ì‹œê°„ ì°¨ì´ ê³„ì‚°
def compare_times(json_words, tg_words):
    results = []
    min_len = min(len(json_words), len(tg_words))

    for i in range(min_len):
        json_start, json_end, json_word = json_words[i]
        tg_start, tg_end, tg_word = tg_words[i]

        start_diff_ms = abs((tg_start - json_start) * 1000)  # ms ë‹¨ìœ„ ë³€í™˜
        end_diff_ms = abs((tg_end - json_end) * 1000)

        results.append([json_word, tg_word, start_diff_ms, end_diff_ms])

    return results

# ê°œë³„ TextGrid íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)
def process_file(textgrid_file):
    results = []
    basename = os.path.basename(textgrid_file).split(".TextGrid")[0]  # "_" ì´ì „ ë¶€ë¶„ì„ basenameìœ¼ë¡œ ì‚¬ìš©

    json_path = json_files.get(basename)  # JSON ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¹ ë¥´ê²Œ ê²€ìƒ‰
    if json_path:
        json_words = parse_json(json_path)
        if json_words:
            tg_words = parse_textgrid(textgrid_file)
            if tg_words:
                matched_results = compare_times(json_words, tg_words)
                for row in matched_results:
                    results.append([textgrid_file] + row)

    return results

# JSON ë° TextGrid íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
json_files = get_json_files()  # JSONì„ {íŒŒì¼ëª…: ê²½ë¡œ} ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
textgrid_files = get_textgrid_files()
orig_words = load_original_words(tsv_original)

# ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰ (num_workers=4ë¡œ ê³ ì •)
if __name__ == "__main__":
    # SAMPLE_SIZE = 0
    # if SAMPLE_SIZE and SAMPLE_SIZE > 0:
    #     textgrid_files = textgrid_files[:SAMPLE_SIZE]
    #     print(f"ğŸ§ª ìƒ˜í”Œ {len(textgrid_files)}ê°œë§Œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # results = []
    # for tg_path in tqdm(textgrid_files, desc="Processing"):
    #     results.extend(process_file(tg_path))

    # # ---------------- ê²°ê³¼ í›„ì²˜ë¦¬ ----------------
    # df = pd.DataFrame(
    #     results,
    #     columns=["íŒŒì¼", "JSON ë‹¨ì–´", "TG ë‹¨ì–´", "ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)", "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"]
    # )
    # df.to_csv("aligner_time_difference_ms.csv", index=False)
    # print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")

    # # ì •í™•ë„/í†µê³„
    # accuracy_data = {"Category": ["Start time alignment", "End time alignment"]}
    # for tol in tolerance_levels:
    #     accuracy_data[f"<{tol}ms"] = [
    #         (df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean(),
    #         (df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean()
    #     ]
    # accuracy_df = pd.DataFrame(accuracy_data)

    # stats = {
    #     "ì´ ìƒ˜í”Œ ê°œìˆ˜": len(df),
    #     "ì‹œì‘ í‰ê·  (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
    #     "ì‹œì‘ í‘œì¤€í¸ì°¨ (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].std(),
    #     "ì¢…ë£Œ í‰ê·  (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
    #     "ì¢…ë£Œ í‘œì¤€í¸ì°¨ (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].std(),
    # }

    # print("\n==== í†µê³„ ë¶„ì„ ê²°ê³¼ ====")
    # for k, v in sã„´tats.items():
    #     print(f"{k}: {v:.4f}")

    # print("\n==== ì •í™•ë„ í…Œì´ë¸” ====")
    # print(accuracy_df.to_string(index=False))

    # accuracy_df.to_csv("aligner_accuracy_table.csv", index=False)
    # print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")
    num_workers = 100  # ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ê³ ì •
    tsv_original = Path("data/dialect_output.tsv")   # ì›ë³¸ TSV ê²½ë¡œ
    orig_words = load_original_words(tsv_original)
    with Pool(num_workers) as pool:
        results = list(tqdm(pool.imap_unordered(process_file, textgrid_files), total=len(textgrid_files)))

    # Flatten the results
    results = [item for sublist in results for item in sublist]

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(results, columns=["íŒŒì¼", "JSON ë‹¨ì–´", "TG ë‹¨ì–´", "ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)", "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"])

    # ê²°ê³¼ ì €ì¥
    df.to_csv("aligner_time_difference_ms.csv", index=False)

    print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")
    
    # ì •í™•ë„ í…Œì´ë¸” ê³„ì‚°
    accuracy_data = {
        "Category": ["Start time alignment", "End time alignment"]
    }

    for tol in tolerance_levels:
        accuracy_data[f"<{tol}ms"] = [
            (df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean(),
            (df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean()
        ]

    # ì •í™•ë„ í…Œì´ë¸” ìƒì„±
    accuracy_df = pd.DataFrame(accuracy_data)

    # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì°¨ì´ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
    stats = {
        "ì´ ìƒ˜í”Œ ê°œìˆ˜": len(df),
        "ì‹œì‘ ì‹œê°„ ì°¨ì´ í‰ê·  (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
        "ì‹œì‘ ì‹œê°„ ì°¨ì´ í‘œì¤€í¸ì°¨ (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].std(),
        "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ í‰ê·  (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
        "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ í‘œì¤€í¸ì°¨ (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].std(),
    }

    # í†µê³„ ë°ì´í„° ì¶œë ¥
    print("\n==== í†µê³„ ë¶„ì„ ê²°ê³¼ ====")
    for key, value in stats.items():
        print(f"{key}: {value:.4f}")

    # ì •í™•ë„ í…Œì´ë¸” ì¶œë ¥
    print("\n==== ì •í™•ë„ í…Œì´ë¸” ====")
    print(accuracy_df.to_string(index=False))

    # ê²°ê³¼ ì €ì¥
    df.to_csv("aligner_time_difference_ms.csv", index=False)
    accuracy_df.to_csv("aligner_accuracy_table.csv", index=False)

    print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")