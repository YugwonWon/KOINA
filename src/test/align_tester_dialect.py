import os
import json
import pickle
import pandas as pd
from textgrid import TextGrid, IntervalTier
from multiprocessing import Pool
from tqdm import tqdm

# JSON ë° TextGrid íŒŒì¼ ëª©ë¡ ì €ì¥ìš© pkl íŒŒì¼
json_pkl_path = "json_file_list.pkl"
tg_pkl_path = "tg_file_list.pkl"

# JSON ë° TextGrid íŒŒì¼ ê²½ë¡œ ì„¤ì •
json_root = "/home/yugwon/nas/audio/ASR/dialect1/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°"
textgrid_root = "/home/yugwon/nas/audio/ASR/dialect1/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training"

# í—ˆìš© ì˜¤ì°¨ (tolerance) ì„¤ì • (ms ë‹¨ìœ„)
tolerance_levels = [10, 25, 50, 100]

# === JSON íŒŒì¼ ëª©ë¡ ê²€ìƒ‰ ë° ì €ì¥ ===
def get_json_files():
    json_files = {}

    if os.path.exists(json_pkl_path):
        print(f"ğŸ“‚ ê¸°ì¡´ JSON íŒŒì¼ ëª©ë¡ ë¡œë“œ ì¤‘: {json_pkl_path}")
        with open(json_pkl_path, "rb") as f:
            json_list = pickle.load(f)  # ê¸°ì¡´ pkl íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ë¨

        # ë¦¬ìŠ¤íŠ¸ë¥¼ {basename: path} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        for json_path in json_list:
            basename = os.path.splitext(os.path.basename(json_path))[0]  # í™•ì¥ì ì œê±°í•˜ì—¬ basename ì¶”ì¶œ
            json_files[basename] = json_path

        return json_files  # ë³€í™˜ëœ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

    # ê¸°ì¡´ pkl íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ê²€ìƒ‰
    json_list = []
    for root, _, files in os.walk(json_root):
        if "ì¶©ì²­ë„" in root:  # 'ì¶©ì²­ë„'ê°€ í¬í•¨ëœ ê²½ë¡œë§Œ í•„í„°ë§
            for file in files:
                if file.endswith(".json"):
                    json_path = os.path.join(root, file)
                    json_list.append(json_path)
                    basename = os.path.splitext(file)[0]
                    json_files[basename] = json_path

    # JSON íŒŒì¼ ëª©ë¡ì„ pickleë¡œ ì €ì¥
    with open(json_pkl_path, "wb") as f:
        pickle.dump(json_list, f)  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥

    print(f"âœ… JSON íŒŒì¼ ëª©ë¡ ì €ì¥ ì™„ë£Œ: {json_pkl_path} ({len(json_files)}ê°œ íŒŒì¼)")
    return json_files


# === TextGrid íŒŒì¼ ëª©ë¡ ê²€ìƒ‰ ë° ì €ì¥ ===
def get_textgrid_files():
    if os.path.exists(tg_pkl_path):
        print(f"ğŸ“‚ ê¸°ì¡´ TextGrid íŒŒì¼ ëª©ë¡ ë¡œë“œ ì¤‘: {tg_pkl_path}")
        with open(tg_pkl_path, "rb") as f:
            return pickle.load(f)

    textgrid_files = []
    for root, _, files in os.walk(textgrid_root):
        if "TL" in root or "TS" in root:  # íŠ¹ì • í´ë” ì œì™¸
            continue
        for file in files:
            if file.endswith(".TextGrid"):
                textgrid_files.append(os.path.join(root, file))

    # TextGrid íŒŒì¼ ëª©ë¡ì„ pickleë¡œ ì €ì¥
    with open(tg_pkl_path, "wb") as f:
        pickle.dump(textgrid_files, f)

    print(f"âœ… TextGrid íŒŒì¼ ëª©ë¡ ì €ì¥ ì™„ë£Œ: {tg_pkl_path} ({len(textgrid_files)}ê°œ íŒŒì¼)")
    return textgrid_files

# JSON íŒŒì¼ì—ì„œ ì–´ì ˆë³„ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì¶œ
def parse_json(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {file_path}")
        return None

    word_segments = []
    for seg in data.get("transcription", {}).get("segments", []):
        try:
            start_time = float(seg["startTime"].split(":")[-1])  # ì´ˆ ë‹¨ìœ„ ë³€í™˜
            end_time = float(seg["endTime"].split(":")[-1])
            word = seg["dialect"]  # ì–´ì ˆ í…ìŠ¤íŠ¸
            word_segments.append((start_time, end_time, word))
        except (KeyError, ValueError):
            print(f"âš ï¸ JSON ë°ì´í„° ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {file_path}")

    return word_segments if word_segments else None

# TextGrid íŒŒì¼ì—ì„œ ì–´ì ˆë³„ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì¶œ
def parse_textgrid(file_path):
    try:
        grid = TextGrid.fromFile(file_path)
    except Exception as e:
        print(f"âŒ TextGrid íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path}, ì˜¤ë¥˜: {e}")
        return None

    word_intervals = []
    for tier in grid.tiers:
        if tier.name == "word" and isinstance(tier, IntervalTier):
            for interval in tier.intervals:
                if interval.mark.strip():  # ë¹ˆ ê³µë°± ì œì™¸
                    word_intervals.append((interval.minTime, interval.maxTime, interval.mark.strip()))
            break

    return word_intervals if word_intervals else None

# JSONê³¼ TextGrid ë¹„êµ í›„ ì‹œê°„ ì°¨ì´ ê³„ì‚°
def compare_times(json_words, tg_words):
    results = []
    min_len = min(len(json_words), len(tg_words))

    for i in range(min_len):
        json_start, json_end, json_word = json_words[i]
        tg_start, tg_end, tg_word = tg_words[i]

        start_diff_ms = abs((tg_start - json_start) * 1000)  # ms ë‹¨ìœ„ ë³€í™˜
        end_diff_ms = abs((tg_end - json_end) * 1000)

        results.append([json_word, tg_word, start_diff_ms, end_diff_ms])

    return results

# ê°œë³„ TextGrid íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)
def process_file(textgrid_file):
    results = []
    basename = os.path.basename(textgrid_file).split("_.TextGrid")[0]  # "_" ì´ì „ ë¶€ë¶„ì„ basenameìœ¼ë¡œ ì‚¬ìš©

    json_path = json_files.get(basename)  # JSON ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¹ ë¥´ê²Œ ê²€ìƒ‰
    if json_path:
        json_words = parse_json(json_path)
        if json_words:
            tg_words = parse_textgrid(textgrid_file)
            if tg_words:
                matched_results = compare_times(json_words, tg_words)
                for row in matched_results:
                    results.append([textgrid_file] + row)

    return results

# JSON ë° TextGrid íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
json_files = get_json_files()  # JSONì„ {íŒŒì¼ëª…: ê²½ë¡œ} ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
textgrid_files = get_textgrid_files()

# ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰ (num_workers=4ë¡œ ê³ ì •)
if __name__ == "__main__":
    num_workers = 100  # ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜ ê³ ì •

    with Pool(num_workers) as pool:
        results = list(tqdm(pool.imap_unordered(process_file, textgrid_files), total=len(textgrid_files)))

    # Flatten the results
    results = [item for sublist in results for item in sublist]

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(results, columns=["íŒŒì¼", "JSON ë‹¨ì–´", "TG ë‹¨ì–´", "ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)", "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"])

    # ê²°ê³¼ ì €ì¥
    df.to_csv("aligner_time_difference_ms.csv", index=False)

    print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")
    
    # ì •í™•ë„ í…Œì´ë¸” ê³„ì‚°
    accuracy_data = {
        "Category": ["Start time alignment", "End time alignment"]
    }

    for tol in tolerance_levels:
        accuracy_data[f"<{tol}ms"] = [
            (df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean(),
            (df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"] < tol).mean()
        ]

    # ì •í™•ë„ í…Œì´ë¸” ìƒì„±
    accuracy_df = pd.DataFrame(accuracy_data)

    # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì°¨ì´ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
    stats = {
        "ì´ ìƒ˜í”Œ ê°œìˆ˜": len(df),
        "ì‹œì‘ ì‹œê°„ ì°¨ì´ í‰ê·  (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
        "ì‹œì‘ ì‹œê°„ ì°¨ì´ í‘œì¤€í¸ì°¨ (ms)": df["ì‹œì‘ ì‹œê°„ ì°¨ì´ (ms)"].std(),
        "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ í‰ê·  (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].mean(),
        "ì¢…ë£Œ ì‹œê°„ ì°¨ì´ í‘œì¤€í¸ì°¨ (ms)": df["ì¢…ë£Œ ì‹œê°„ ì°¨ì´ (ms)"].std(),
    }

    # í†µê³„ ë°ì´í„° ì¶œë ¥
    print("\n==== í†µê³„ ë¶„ì„ ê²°ê³¼ ====")
    for key, value in stats.items():
        print(f"{key}: {value:.4f}")

    # ì •í™•ë„ í…Œì´ë¸” ì¶œë ¥
    print("\n==== ì •í™•ë„ í…Œì´ë¸” ====")
    print(accuracy_df.to_string(index=False))

    # ê²°ê³¼ ì €ì¥
    df.to_csv("aligner_time_difference_ms.csv", index=False)
    accuracy_df.to_csv("aligner_accuracy_table.csv", index=False)

    print("\n==== CSV ì €ì¥ ì™„ë£Œ ====")
