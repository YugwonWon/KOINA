import os
import pickle
import numpy as np
import pandas as pd
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import Counter

import shap

from torch.utils.data import ConcatDataset, TensorDataset
from torch.utils.data import DataLoader, Dataset, Subset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

import logging
import matplotlib.pyplot as plt

from imblearn.under_sampling import RandomUnderSampler

import itertools
import seaborn as sns
import matplotlib.colors as mcolors
from scipy import stats
from statsmodels.stats.multitest import multipletests

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = "out/models"
LOG_FILE_PATH = "out/logs/train.log"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    filename=LOG_FILE_PATH,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("train")
if not logger.handlers:
    logger.addHandler(logging.StreamHandler())  # ì½˜ì†” ì¶œë ¥ ì¶”ê°€

# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ ê°€ëŠ¥)
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# TensorBoard ì„¤ì •
log_dir = os.path.join(OUTPUT_DIR, "tensorboard_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
writer = SummaryWriter(log_dir=log_dir)

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class SpeechDataset(Dataset):
    def __init__(self, csv_file):
        df = pd.read_csv(csv_file)

        # ì…ë ¥(X), ì¶œë ¥(Y) ì„¤ì •
        self.feature_names = ["start_f0", "end_f0", "mean_f0", "max_f0", "min_f0", "slope", "TCoG"]
        self.X = df[self.feature_names].values
        self.y = df["cluster_label"].values  # 0 = í•˜ê°•, 1 = ìƒìŠ¹
        self.filenames = df["filename"].values  # íŒŒì¼ëª… ì¶”ê°€
        
        # ë°ì´í„° ì •ê·œí™” -> load_data()ì—ì„œ ì²˜ë¦¬
        # self.scaler = StandardScaler()
        # self.X = self.scaler.fit_transform(self.X)
        
        # Tensor ë³€í™˜
        self.X = torch.tensor(self.X, dtype=torch.float32)
        self.y = torch.tensor(self.y, dtype=torch.long)

        # ì›ë³¸ ë°ì´í„° ì €ì¥ (ì¸µí™” ì¶”ì¶œìš©)
        self.df = df  

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# MLP ëª¨ë¸ ì •ì˜
class MLP(nn.Module):
    def __init__(self, input_dim, dropout_rate=0.2):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),  # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ ì´í›„ dropout
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate),  # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ ì´í›„ dropout
            nn.Linear(32, 1)  # ì¶œë ¥ í´ë˜ìŠ¤ 2ê°œ (í•˜ê°•/ìƒìŠ¹)
        )
    
    def forward(self, x):
        return self.layers(x)


# ë°ì´í„° ë¡œë“œ & ì¸µí™” ìƒ˜í”Œë§ ì ìš© + Undersampling (ì„±ë³„ ì •ë³´ ë°˜ì˜)

def load_data_mixed(pseudo_csv, human_csv, batch_size=32):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pseudo_df = pd.read_csv(pseudo_csv)
    human_df  = pd.read_csv(human_csv)

    feature_names = ["start_f0","end_f0","mean_f0","max_f0",
                     "min_f0","slope","TCoG"]

    # â”€â”€â”€â”€â”€ 1) ì¸ê°„ ë¼ë²¨ 6-2-2 ë¶„í•  â”€â”€â”€â”€â”€
    gender_h = human_df["filename"].str[-1].values
    y_h      = human_df["cluster_label"].values
    strat_h  = np.array([f"{y}_{g}" for y,g in zip(y_h, gender_h)])

    idx_h = np.arange(len(human_df))
    sss1  = StratifiedShuffleSplit(1, test_size=0.40, random_state=SEED)
    train_h_idx, temp_h_idx = next(sss1.split(idx_h, strat_h))

    strat_temp = strat_h[temp_h_idx]
    sss2  = StratifiedShuffleSplit(1, test_size=0.50, random_state=SEED)
    valid_rel, test_rel = next(sss2.split(temp_h_idx, strat_temp))
    valid_h_idx, test_h_idx = temp_h_idx[valid_rel], temp_h_idx[test_rel]

    # â”€â”€â”€â”€â”€ 2) êµ°ì§‘ ë¼ë²¨(90 %)ì€ train ì „ìš© â”€â”€â”€â”€â”€
    gender_p = pseudo_df["filename"].str[-1].values
    y_p      = pseudo_df["cluster_label"].values
    strat_p  = np.array([f"{y}_{g}" for y,g in zip(y_p, gender_p)])
    idx_p    = np.arange(len(pseudo_df))

    rus = RandomUnderSampler(random_state=SEED)
    train_p_idx, _ = rus.fit_resample(idx_p.reshape(-1,1), strat_p)
    train_p_idx = train_p_idx.flatten()

    # â”€â”€â”€â”€â”€ 3) ìŠ¤ì¼€ì¼ë§ â”€â”€â”€â”€â”€
    X_p = pseudo_df[feature_names].values.astype(np.float32)
    X_h = human_df[feature_names].values.astype(np.float32)

    scaler = StandardScaler().fit(
        np.vstack([X_p[train_p_idx], X_h[train_h_idx]])
    )

    def make_tensor(idx_list, X_arr, y_arr):
        X = torch.tensor(scaler.transform(X_arr[idx_list]))
        y = torch.tensor(y_arr[idx_list])
        return TensorDataset(X, y)

    train_ds = ConcatDataset([
        make_tensor(train_p_idx, X_p, y_p),   # êµ°ì§‘
        make_tensor(train_h_idx, X_h, y_h)    # ì¸ê°„(60 %)
    ])
    valid_ds = make_tensor(valid_h_idx, X_h, y_h)
    test_ds  = make_tensor(test_h_idx,  X_h, y_h)

    train_loader = DataLoader(train_ds, batch_size, shuffle=True,  drop_last=False)
    valid_loader = DataLoader(valid_ds, batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds,  batch_size, shuffle=False)

    # â”€â”€â”€â”€â”€ 4) í‰ê°€ìš© íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ â”€â”€â”€â”€â”€
    valid_fnames = human_df["filename"].values[valid_h_idx]
    test_fnames  = human_df["filename"].values[test_h_idx]

    human_dataset = SpeechDataset(human_csv)   # íŒŒì¼ëª…Â·ë¼ë²¨ ì „ì²´ ë³´ì¡´
    
    def print_dist(name, idx_list, y_arr, fnames):
        labels = y_arr[idx_list]
        genders= [fname[-1] for fname in fnames[idx_list]]
        combined = [f"{y}_{g}" for y,g in zip(labels, genders)]
        cnt = Counter(combined)
        total = len(idx_list)
        print(f"â–¶ {name} set: {total} samples â€” ë¶„í¬:", end=" ")
        print(", ".join(f"{k}:{v}" for k,v in cnt.items()))

    print_dist("Train(pseudo)", train_p_idx, y_p, pseudo_df["filename"].values)
    print_dist("Train(human)", train_h_idx, y_h,    human_df["filename"].values)
    print_dist("Valid",       valid_h_idx, y_h,    human_df["filename"].values)
    print_dist("Test",        test_h_idx,  y_h,    human_df["filename"].values)

    return (train_loader, valid_loader, test_loader,
            human_dataset, valid_fnames, test_fnames)

# Feature Importance ë¶„ì„ ë° ì‹œê°í™”
def plot_feature_importance(model):
    import seaborn as sns
    import matplotlib.colors as mcolors

    feature_weights = model.layers[0].weight.detach().cpu().numpy()
    feature_importance = np.abs(feature_weights).mean(axis=0)  # í‰ê·  ì ˆëŒ€ê°’ ì‚¬ìš©
    feature_names = ["start_f0", "end_f0", "mean_f0", "max_f0", "min_f0", "slope", "TCoG"]

    # ë°” ê·¸ë˜í”„ (ë‚´ë¦¼ì°¨ìˆœ + ê·¸ë¼ë°ì´ì…˜ ì ìš©)
    sorted_indices = np.argsort(feature_importance)[::-1]
    sorted_features = [feature_names[i] for i in sorted_indices]
    sorted_importance = feature_importance[sorted_indices]

    plt.figure(figsize=(10, 6))
    cmap = mcolors.LinearSegmentedColormap.from_list("", ["navy", "skyblue"])  # ì§„í•œ ìƒ‰ â†’ ì˜…ì€ ìƒ‰
    colors = [cmap(i / len(sorted_importance)) for i in range(len(sorted_importance))]
    bars = plt.barh(sorted_features, sorted_importance, color=colors)
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.title("Feature Importance in MLP Model")

    # ë°” ìœ„ì— ê°’ ì¶”ê°€
    for bar, imp in zip(bars, sorted_importance):
        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2, f"{imp:.3f}", va="center")

    plt.gca().invert_yaxis()  # ìœ„ì—ì„œ ì•„ë˜ë¡œ ì •ë ¬
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_importance_bar.png"))
    logger.info("ğŸ“Š Feature importance bar chart saved as 'out/models/feature_importance_bar.png'")

    # ìƒê´€ê´€ê³„ í”Œë¡¯
    correlation_matrix = np.corrcoef(feature_weights.T)
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        correlation_matrix,
        annot=True,
        cmap="coolwarm",
        xticklabels=feature_names,
        yticklabels=feature_names,
        cbar_kws={"label": "Correlation"},
        fmt=".2f"
    )
    plt.title("Feature Correlation Matrix")
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_correlation_matrix.png"))
    logger.info("ğŸ“Š Feature correlation matrix saved as 'out/models/feature_correlation_matrix.png'")

    # ë ˆì´ë” ì°¨íŠ¸ (íŠ¹ì„±ë³„ í‰ê·  ì¤‘ìš”ë„)
    normalized_importance = sorted_importance / max(sorted_importance)
    angles = np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False).tolist()
    feature_importance_circle = np.concatenate((normalized_importance, [normalized_importance[0]]))
    angles += angles[:1]

    # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw={"polar": True})
    ax.fill(angles, feature_importance_circle, color="skyblue", alpha=0.4)
    ax.plot(angles, feature_importance_circle, color="blue", linewidth=2)

    # ë°˜ì§€ë¦„ ì¶• ì„¤ì • (ì •ê·œí™”ëœ ì¤‘ìš”ë„ì— ëŒ€ì‘)
    ax.set_yticks(np.linspace(0, 1, 5))  # ì •ê·œí™”ëœ ë²”ìœ„ë¡œ ë°˜ì§€ë¦„ ì„¤ì •
    ax.set_yticklabels([f"{v:.2f}" for v in np.linspace(0, max(sorted_importance), 5)])  # ì›ë˜ ì¤‘ìš”ë„ ê°’ í‘œì‹œ
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(sorted_features)

    # ì œëª©ê³¼ ì €ì¥
    ax.set_title("Radar Chart of Feature Importance (Normalized)", va="bottom")
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_importance_radar_normalized.png"))
    logger.info("ğŸ“Š Feature importance radar chart saved as 'out/models/feature_importance_radar_normalized.png'")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í†µê³„ ê²€ì • ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) neuron Ã— feature í–‰ë ¬ì—ì„œ ì ˆëŒ€ê°’ ë¶„í¬ ì¶”ì¶œ
    W = model.layers[0].weight.detach().cpu().numpy()      # shape (n_neurons, 7)
    absW = np.abs(W)                                       # (n_neurons, 7)
    order = ["end_f0","max_f0","mean_f0","min_f0",
             "start_f0","slope","TCoG"]
    idx   = [feature_names.index(f) for f in order]
    data  = absW[:, idx]                                   # (n_neurons,7)

    # 2) Friedman ê²€ì •Â¹: 7ê°œ ê·¸ë£¹ ê°„ ë¶„í¬ ì°¨ì´ í™•ì¸
    stat, p = stats.friedmanchisquare(*[data[:,i] for i in range(data.shape[1])])
    print(f"â–¶ Friedman Ï‡Â² = {stat:.3f}, p = {p:.3e}")

    # 3) ì‚¬í›„ê²€ì •: Wilcoxon ë¶€í˜¸ê²€ì • + Holmâ€“Bonferroni ë³´ì •Â²
    pairs   = list(itertools.combinations(range(len(order)), 2))
    pvals   = []
    pairs_nm= []
    for i,j in pairs:
        _, p_ij = stats.wilcoxon(data[:,i], data[:,j])
        pvals.append(p_ij)
        pairs_nm.append(f"{order[i]} vs {order[j]}")
    rej, p_corr, _, _ = multipletests(pvals, alpha=0.05, method="holm")

    # ê²°ê³¼ ì¶œë ¥
    for name, p0, p1, r in zip(pairs_nm, pvals, p_corr, rej):
        sig = "ìœ ì˜" if r else "ns"
        print(f"  - {name}: unadj p={p0:.3e}, adj p={p1:.3e} â†’ {sig}")
    
# í•™ìŠµ ê³¼ì • ì‹œê°í™” í•¨ìˆ˜
def plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies):
    epochs = range(1, len(train_losses) + 1)

    # Loss ê³¡ì„ 
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, valid_losses, label="Valid Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "loss_curve.png"))
    logger.info("ğŸ“ˆ Loss curve saved as 'loss_curve.png'")

    # Accuracy ê³¡ì„ 
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_accuracies, label="Train Accuracy")
    plt.plot(epochs, valid_accuracies, label="Valid Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy Curve")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "accuracy_curve.png"))
    logger.info("ğŸ“ˆ Accuracy curve saved as 'accuracy_curve.png'")
    
# SHAP ë¶„ì„ ë° ì‹œê°í™” í•¨ìˆ˜ (Checkpointing ì¶”ê°€)
def plot_shap_analysis_combined(model,
                                valid_loader,
                                test_loader,
                                valid_fnames,          # ë¦¬ìŠ¤íŠ¸
                                test_fnames,           # ë¦¬ìŠ¤íŠ¸
                                output_dir="out/models"):

    # 1) ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜
    def model_predict(X):
        with torch.no_grad():
            x = torch.tensor(X, dtype=torch.float32,
                             device=next(model.parameters()).device)
            return model(x).cpu().numpy()

    # 2) ValidÂ·Test ë¬¶ì–´ì„œ í–‰ë ¬/ë¼ë²¨/íŒŒì¼ëª… ë§Œë“¤ê¸°
    X_chunks, y_chunks, fname_chunks = [], [], []

    # helper: ë¡œë”ì™€ ëŒ€ì‘í•˜ëŠ” íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë‹¤ë£¬ë‹¤
    for loader, fnames in zip((valid_loader, test_loader),
                              (valid_fnames, test_fnames)):
        ptr = 0
        for X_b, y_b in loader:
            bs = len(y_b)
            X_chunks.append(X_b.numpy())
            y_chunks.append(y_b.numpy())
            fname_chunks.extend(fnames[ptr:ptr+bs])
            ptr += bs

    X_all = np.vstack(X_chunks)           # (N, 7)
    y_all = np.hstack(y_chunks)           # (N,)
    filenames = fname_chunks              # ê¸¸ì´ N

    feature_names = np.array([
        "start_f0", "end_f0", "mean_f0",
        "max_f0", "min_f0", "slope", "TCoG"
    ])

    # 3) SHAP ê°’ ê³„ì‚° ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    ckpt = os.path.join(output_dir, "shap_values_combined.pkl")
    if os.path.exists(ckpt):
        with open(ckpt, "rb") as f:
            shap_values = pickle.load(f)
        explainer = None
        base_val  = 0
    else:
        explainer   = shap.KernelExplainer(model_predict, X_all[:100])
        shap_values = explainer.shap_values(X_all)   # (N, 7, 1)  â† ì—¬ê¸°!
        if isinstance(shap_values, list):            # ë¦¬ìŠ¤íŠ¸ì¼ ë•Œ
            shap_values = shap_values[0]             # (N, 7, 1)
        # ìƒˆë¡œ ì¶”ê°€: ë¶ˆí•„ìš”í•œ 3ë²ˆì§¸ ì¶• ì œê±°
        if shap_values.ndim == 3 and shap_values.shape[-1] == 1:
            shap_values = shap_values.squeeze(-1)    # (N, 7)
        base_val = explainer.expected_value
        with open(ckpt, "wb") as f:
            pickle.dump(shap_values, f)

    # 4) Summary plot
    exp = shap.Explanation(
    values        = shap_values,     # (N, 7)
    base_values   = np.repeat(base_val, len(shap_values)),
    data          = X_all,           # (N, 7)
    feature_names = feature_names
    )
    plt.figure(figsize=(8, 6))
    shap.plots.beeswarm(exp, max_display=7, show=False)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "shap_summary_all.png"))
    plt.close()

    # 5) í´ë˜ìŠ¤ë³„ waterfall ìƒ˜í”Œ 50ê°œì”© ---------------------------------
    def save_waterfall(idx, label_str):
        expl = shap.Explanation(
            values        = shap_values[idx],
            base_values   = base_val,
            data          = X_all[idx],
            feature_names = feature_names
        )
        plt.figure(figsize=(6, 4))
        shap.plots.waterfall(expl, max_display=7, show=False)
        plt.title(f"{filenames[idx]} ({label_str})")
        plt.tight_layout()
        fname_png = f"shap_waterfall_{label_str}_{filenames[idx]}.png"
        plt.savefig(os.path.join(output_dir, fname_png))
        plt.close()

    class_0_idx = np.where(y_all == 0)[0][:50]
    class_1_idx = np.where(y_all == 1)[0][:50]

    for i in class_0_idx:
        save_waterfall(i, "downward")
    for i in class_1_idx:
        save_waterfall(i, "upward")
        
    def save_beeswarm(shap_vals_subset, X_subset, idx_subset, label_str):
        """í´ë˜ìŠ¤ë³„ beeswarm summary ì €ì¥"""
        exp = shap.Explanation(
            values        = shap_vals_subset,                     # (n_k, 7)
            base_values   = np.repeat(base_val, len(idx_subset)), # ê¸¸ì´ n_k
            data          = X_subset,                             # (n_k, 7)
            feature_names = feature_names
        )
        plt.figure(figsize=(8, 6))
        shap.plots.beeswarm(exp, max_display=7, show=False)       # ì‹  API â†’ spine ë²„ê·¸ ì—†ìŒ
        plt.title(f"SHAP Beeswarm ({label_str})")
        plt.tight_layout()
        fname = f"shap_beeswarm_{label_str}.png"
        plt.savefig(os.path.join(output_dir, fname))
        plt.close()

    # â‘  í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤
    class_0_idx = np.where(y_all == 0)[0]
    class_1_idx = np.where(y_all == 1)[0]

    # â‘¡ 0-í´ë˜ìŠ¤ summary
    save_beeswarm(
        shap_vals_subset = shap_values[class_0_idx],
        X_subset         = X_all[class_0_idx],
        idx_subset       = class_0_idx,
        label_str        = "downward"
    )

    # â‘¢ 1-í´ë˜ìŠ¤ summary
    save_beeswarm(
        shap_vals_subset = shap_values[class_1_idx],
        X_subset         = X_all[class_1_idx],
        idx_subset       = class_1_idx,
        label_str        = "upward"
    )

    print("SHAP summary & waterfall plots have been saved.")
    
# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •: Loss/Accuracy ê¸°ë¡)
def train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=120, checkpoint_path="out/models/best_checkpoint.pth"):
    best_valid_loss = float("inf")
    train_losses, valid_losses = [], []
    train_accuracies, valid_accuracies = [], []

    for epoch in range(num_epochs):
        model.train()
        total_loss, correct = 0, 0
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            # output = model(X_batch) shape (batch, 2)
            # loss = criterion(output, y_batch) integer class label
            logits = model(X_batch).squeeze(1)  # shape (batch,)
            loss = criterion(logits, y_batch.float())
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            # correct += (output.argmax(1) == y_batch).sum().item()
            probs   = torch.sigmoid(logits)
            preds   = (probs >= 0.5).long()
            correct += (preds == y_batch).sum().item()

        train_acc = correct / len(train_loader.dataset)
        train_losses.append(total_loss / len(train_loader))
        train_accuracies.append(train_acc)
        
        writer.add_scalar("Loss/train", total_loss / len(train_loader), epoch)
        writer.add_scalar("Accuracy/train", train_acc, epoch)
        
        model.eval()
        valid_loss, valid_correct = 0, 0
        with torch.no_grad():
            for X_batch, y_batch in valid_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                
                # 1) ë¡œì§“ ì¶”ì¶œ (shape: [batch,])
                logits = model(X_batch).squeeze(-1)
                
                # 2) loss ê³„ì‚° (float íƒ€ì… ë ˆì´ë¸”)
                loss = criterion(logits, y_batch.float())
                valid_loss += loss.item()
                
                # 3) í™•ë¥  ë³€í™˜ & ì˜ˆì¸¡
                probs = torch.sigmoid(logits)
                preds = (probs >= 0.5).long()
                valid_correct += (preds == y_batch).sum().item()

        # 4) ì§€í‘œ ì§‘ê³„
        valid_acc = valid_correct / len(valid_loader.dataset)
        valid_losses.append(valid_loss / len(valid_loader))
        valid_accuracies.append(valid_acc)

        # 5) TensorBoard ê¸°ë¡
        writer.add_scalar("Loss/valid", valid_loss / len(valid_loader), epoch)
        writer.add_scalar("Accuracy/valid", valid_acc, epoch)

        # 6) ë¡œê·¸ ì¶œë ¥ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        logger.info(
            f"Epoch [{epoch+1}/{num_epochs}] "
            f"Train Loss: {total_loss:.4f}, Valid Loss: {valid_loss:.4f}, "
            f"Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}"
        )
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), checkpoint_path)
            logger.info(f"âœ” Best model saved at epoch {epoch+1}")

    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)

# ì„±ëŠ¥ ë¦¬í¬íŠ¸ (Test ê²°ê³¼)
def evaluate_model(model, test_loader, device, dataset):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            # output = model(X_batch)
            # predictions = output.argmax(1).cpu().numpy()
            logits = model(X_batch).squeeze(1) # (batch,)
            probs  = torch.sigmoid(logits)
            predictions = (probs >= 0.5).long().cpu().numpy()
            y_pred.extend(predictions)
            y_true.extend(y_batch.cpu().numpy())

    # Classification Report
    report = classification_report(y_true, y_pred, target_names=["Falling", "Rising"], output_dict=True)
    logger.info(f"Test Report:\n{pd.DataFrame(report).transpose()}")
    report_path = os.path.join(OUTPUT_DIR, "classification_report.csv")
    pd.DataFrame(report).transpose().to_csv(report_path)
    logger.info(f"ğŸ“Š Classification report saved as '{report_path}'")
    

# ì‹¤í–‰
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë“œ
    train_loader, valid_loader, test_loader, human_ds, \
    valid_fnames, test_fnames = load_data_mixed(
        pseudo_csv="train-data/training_data.csv",
        human_csv="train-data/training_data_human.csv",
        batch_size=32
    )

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = MLP(input_dim=7).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_model(model, train_loader, valid_loader,
                criterion, optimizer, device, num_epochs=2000)
    evaluate_model(model, test_loader, device, None) 
    
    
    # # ë°ì´í„° ë¡œë“œ (í•™ìŠµ ì—†ì´ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ)
    # train_loader, valid_loader, test_loader, human_ds, \
    # valid_fnames, test_fnames = load_data_mixed(
    #     pseudo_csv="train-data/training_data.csv",
    #     human_csv="train-data/training_data_human.csv",
    #     batch_size=32
    # )

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì´ˆê¸°í™” í•„ìš”)
    model = MLP(input_dim=7).to(device)

    # ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    checkpoint_path = "out/models/best_checkpoint.pth"  # ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
    if os.path.exists(checkpoint_path):
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        logger.info(f"Model checkpoint loaded from '{checkpoint_path}'")
    else:
        raise FileNotFoundError(f"Error: No checkpoint found at '{checkpoint_path}'")

    # ì‹œê°í™” í•¨ìˆ˜ ì‹¤í–‰ (í•™ìŠµ ì—†ì´ ë°”ë¡œ ë¶„ì„ ì§„í–‰)
    plot_feature_importance(model)
    # ëª¨ë¸Â·ì²´í¬í¬ì¸íŠ¸ ë¡œë”© í›„
    plot_shap_analysis_combined(
        model,
        valid_loader,
        test_loader,
        valid_fnames,
        test_fnames,
        output_dir="out/models"
    )

s