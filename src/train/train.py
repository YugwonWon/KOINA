import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import shap
from torch.utils.data import DataLoader, Dataset, Subset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report
import random
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.under_sampling import RandomUnderSampler

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = "out/models"
LOG_FILE_PATH = "out/logs/train.log"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    filename=LOG_FILE_PATH,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("train")
if not logger.handlers:
    logger.addHandler(logging.StreamHandler())  # ì½˜ì†” ì¶œë ¥ ì¶”ê°€

# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ ê°€ëŠ¥)
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# TensorBoard ì„¤ì •
log_dir = os.path.join(OUTPUT_DIR, "tensorboard_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
writer = SummaryWriter(log_dir=log_dir)

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class SpeechDataset(Dataset):
    def __init__(self, csv_file):
        df = pd.read_csv(csv_file)

        # ì…ë ¥(X), ì¶œë ¥(Y) ì„¤ì •
        self.feature_names = ["start_f0", "end_f0", "mean_f0", "max_f0", "min_f0", "slope", "TCoG"]
        self.X = df[self.feature_names].values
        self.y = df["cluster_label"].values  # 0 = í•˜ê°•, 1 = ìƒìŠ¹
        self.filenames = df["filename"].values  # íŒŒì¼ëª… ì¶”ê°€
        
        # ë°ì´í„° ì •ê·œí™” -> load_data()ì—ì„œ ì²˜ë¦¬
        # self.scaler = StandardScaler()
        # self.X = self.scaler.fit_transform(self.X)
        
        # Tensor ë³€í™˜
        self.X = torch.tensor(self.X, dtype=torch.float32)
        self.y = torch.tensor(self.y, dtype=torch.long)

        # ì›ë³¸ ë°ì´í„° ì €ì¥ (ì¸µí™” ì¶”ì¶œìš©)
        self.df = df  

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# MLP ëª¨ë¸ ì •ì˜
class MLP(nn.Module):
    def __init__(self, input_dim, dropout_rate=0.2):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout_rate),  # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ ì´í›„ dropout
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate),  # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ ì´í›„ dropout
            nn.Linear(32, 2)  # ì¶œë ¥ í´ë˜ìŠ¤ 2ê°œ (í•˜ê°•/ìƒìŠ¹)
        )
    
    def forward(self, x):
        return self.layers(x)


# ë°ì´í„° ë¡œë“œ & ì¸µí™” ìƒ˜í”Œë§ ì ìš© + Undersampling (ì„±ë³„ ì •ë³´ ë°˜ì˜)
def load_data(csv_file, batch_size=32):
    from torch.utils.data import TensorDataset

    # --- ê¸°ì¡´ stratified split & undersampling ë¡œì§ ê·¸ëŒ€ë¡œ ---
    dataset = SpeechDataset(csv_file)  
    feature_names = dataset.feature_names
    df = dataset.df  # raw pandas.DataFrame

    # ë ˆì´ë¸”Â·ì„±ë³„ ê²°í•© stratificationìš©
    gender = np.array([fname[-1] for fname in dataset.filenames])
    y_all = dataset.y.numpy()
    strat_labels = np.array([f"{y}_{g}" for y, g in zip(y_all, gender)])

    idx = np.arange(len(dataset))
    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED)
    train_idx, temp_idx = next(sss1.split(idx, strat_labels))

    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)
    temp_strat = strat_labels[temp_idx]
    valid_rel, test_rel = next(sss2.split(temp_idx, temp_strat))
    valid_idx = temp_idx[valid_rel]
    test_idx  = temp_idx[test_rel]

    # Trainë§Œ undersampling
    rus = RandomUnderSampler(random_state=SEED)
    train_idx_res, _ = rus.fit_resample(train_idx.reshape(-1,1), strat_labels[train_idx])
    train_idx = train_idx_res.flatten()

    # --- ì—¬ê¸°ê°€ í•µì‹¬: raw feature matrix êº¼ë‚´ê¸° ---
    X_all = df[feature_names].values.astype(np.float32)  # shape (N,7)

    # (1) Trainìœ¼ë¡œë§Œ scaler fit
    scaler = StandardScaler().fit(X_all[train_idx])

    # (2) Train/Valid/Test ê°ê° transform
    X_train = scaler.transform(X_all[train_idx])
    X_valid = scaler.transform(X_all[valid_idx])
    X_test  = scaler.transform(X_all[test_idx])

    y_train = y_all[train_idx]
    y_valid = y_all[valid_idx]
    y_test  = y_all[test_idx]

    # (3) TensorDatasetìœ¼ë¡œ ë¬¶ê¸°
    train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))
    valid_ds = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))
    test_ds  = TensorDataset(torch.tensor(X_test),  torch.tensor(y_test))

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(valid_ds, batch_size=batch_size)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size)

    # ë°ì´í„°ì…‹ ê°œìˆ˜ ë° ë ˆì´ë¸” ë¶„í¬ ë¡œê¹… (ê²°í•© ë ˆì´ë¸” "y_gender" ê¸°ì¤€)
    def log_label_distribution_from_idx(name, idx_list):
        from collections import Counter
        # ì›ë³¸ datasetì—ì„œ ê° ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” yì™€ filename[-1]ì„ ë½‘ì•„ì„œ ê²°í•©
        combined = [
            f"{dataset.y[idx].item()}_{dataset.filenames[idx][-1]}"
            for idx in idx_list
        ]
        counts = Counter(combined)
        dist = ", ".join(f"{lab}: {cnt}" for lab, cnt in counts.items())
        logger.info(f"ğŸ“ {name} ë ˆì´ë¸” ë¶„í¬ (y_gender): {dist}")

    # ì–¸ë”ìƒ˜í”Œë§ í›„ train_idxê°€ ì¬ì •ì˜ ë˜ì—ˆìœ¼ë‹ˆ, ë¡œê·¸ëŠ” ì—¬ê¸°ì„œ
    log_label_distribution_from_idx("Train", train_idx)
    log_label_distribution_from_idx("Valid", valid_idx)
    log_label_distribution_from_idx("Test",  test_idx)

    return train_loader, valid_loader, test_loader, dataset

# Feature Importance ë¶„ì„ ë° ì‹œê°í™”
def plot_feature_importance(model, dataset):
    import seaborn as sns
    import matplotlib.colors as mcolors

    feature_weights = model.layers[0].weight.detach().cpu().numpy()
    feature_importance = np.abs(feature_weights).mean(axis=0)  # í‰ê·  ì ˆëŒ€ê°’ ì‚¬ìš©
    feature_names = ["start_f0", "end_f0", "mean_f0", "max_f0", "min_f0", "slope", "TCoG"]

    # ë°” ê·¸ë˜í”„ (ë‚´ë¦¼ì°¨ìˆœ + ê·¸ë¼ë°ì´ì…˜ ì ìš©)
    sorted_indices = np.argsort(feature_importance)[::-1]
    sorted_features = [feature_names[i] for i in sorted_indices]
    sorted_importance = feature_importance[sorted_indices]

    plt.figure(figsize=(10, 6))
    cmap = mcolors.LinearSegmentedColormap.from_list("", ["navy", "skyblue"])  # ì§„í•œ ìƒ‰ â†’ ì˜…ì€ ìƒ‰
    colors = [cmap(i / len(sorted_importance)) for i in range(len(sorted_importance))]
    bars = plt.barh(sorted_features, sorted_importance, color=colors)
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.title("Feature Importance in MLP Model")

    # ë°” ìœ„ì— ê°’ ì¶”ê°€
    for bar, imp in zip(bars, sorted_importance):
        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2, f"{imp:.3f}", va="center")

    plt.gca().invert_yaxis()  # ìœ„ì—ì„œ ì•„ë˜ë¡œ ì •ë ¬
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_importance_bar.png"))
    logger.info("ğŸ“Š Feature importance bar chart saved as 'out/models/feature_importance_bar.png'")

    # ìƒê´€ê´€ê³„ í”Œë¡¯
    correlation_matrix = np.corrcoef(feature_weights.T)
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        correlation_matrix,
        annot=True,
        cmap="coolwarm",
        xticklabels=feature_names,
        yticklabels=feature_names,
        cbar_kws={"label": "Correlation"},
        fmt=".2f"
    )
    plt.title("Feature Correlation Matrix")
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_correlation_matrix.png"))
    logger.info("ğŸ“Š Feature correlation matrix saved as 'out/models/feature_correlation_matrix.png'")

    # ë ˆì´ë” ì°¨íŠ¸ (íŠ¹ì„±ë³„ í‰ê·  ì¤‘ìš”ë„)
    normalized_importance = sorted_importance / max(sorted_importance)
    angles = np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False).tolist()
    feature_importance_circle = np.concatenate((normalized_importance, [normalized_importance[0]]))
    angles += angles[:1]

    # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw={"polar": True})
    ax.fill(angles, feature_importance_circle, color="skyblue", alpha=0.4)
    ax.plot(angles, feature_importance_circle, color="blue", linewidth=2)

    # ë°˜ì§€ë¦„ ì¶• ì„¤ì • (ì •ê·œí™”ëœ ì¤‘ìš”ë„ì— ëŒ€ì‘)
    ax.set_yticks(np.linspace(0, 1, 5))  # ì •ê·œí™”ëœ ë²”ìœ„ë¡œ ë°˜ì§€ë¦„ ì„¤ì •
    ax.set_yticklabels([f"{v:.2f}" for v in np.linspace(0, max(sorted_importance), 5)])  # ì›ë˜ ì¤‘ìš”ë„ ê°’ í‘œì‹œ
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(sorted_features)

    # ì œëª©ê³¼ ì €ì¥
    ax.set_title("Radar Chart of Feature Importance (Normalized)", va="bottom")
    plt.tight_layout()
    plt.savefig(os.path.join("out/models", "feature_importance_radar_normalized.png"))
    logger.info("ğŸ“Š Feature importance radar chart saved as 'out/models/feature_importance_radar_normalized.png'")
    
# í•™ìŠµ ê³¼ì • ì‹œê°í™” í•¨ìˆ˜
def plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies):
    epochs = range(1, len(train_losses) + 1)

    # Loss ê³¡ì„ 
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, valid_losses, label="Valid Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss Curve")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "loss_curve.png"))
    logger.info("ğŸ“ˆ Loss curve saved as 'loss_curve.png'")

    # Accuracy ê³¡ì„ 
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_accuracies, label="Train Accuracy")
    plt.plot(epochs, valid_accuracies, label="Valid Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy Curve")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "accuracy_curve.png"))
    logger.info("ğŸ“ˆ Accuracy curve saved as 'accuracy_curve.png'")

# SHAP ë¶„ì„ ë° ì‹œê°í™” í•¨ìˆ˜ (Checkpointing ì¶”ê°€)
def plot_shap_analysis_combined(model, valid_loader, test_loader, dataset, output_dir="out/models"):

    # ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
    def model_predict(X):
        model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X, dtype=torch.float32).to(next(model.parameters()).device)
            logits = model(X_tensor).cpu().numpy()
            return logits

    # Validì™€ Test ë°ì´í„° ë³‘í•©
    data_list = []
    y_list = []
    filenames = []

    for loader in [valid_loader, test_loader]:
        for X_batch, y_batch in loader:
            data_list.append(X_batch.numpy())
            y_list.append(y_batch.numpy())
            filenames += [dataset.filenames[idx] for idx in range(len(y_batch))]

    X_combined = np.concatenate(data_list, axis=0)  # Valid + Test ë°ì´í„°
    y_combined = np.concatenate(y_list, axis=0)  # Valid + Test ë¼ë²¨
    feature_names = ["start_f0", "end_f0", "mean_f0", "max_f0", "min_f0", "slope", "TCoG"]

    # Checkpoint íŒŒì¼ ê²½ë¡œ
    checkpoint_path = os.path.join(output_dir, "shap_values_combined.pkl")

    # SHAP Explainer ìƒì„±
    if os.path.exists(checkpoint_path):
        logger.info(f"SHAP values checkpoint found. Loading from {checkpoint_path}.")
        with open(checkpoint_path, "rb") as f:
            shap_values = pickle.load(f)
        explainer = None  # ì €ì¥ëœ SHAP ê°’ ì‚¬ìš© ì‹œ explainerëŠ” í•„ìš” ì—†ìŒ
    else:
        logger.info(f"Calculating SHAP values for combined dataset. This may take some time...")
        explainer = shap.KernelExplainer(model_predict, X_combined)  # ì²« 100ê°œ ìƒ˜í”Œë¡œ ë°°ê²½ ë°ì´í„° êµ¬ì„±
        shap_values = np.array(explainer.shap_values(X_combined))  # SHAP ê°’ ê³„ì‚°

        # ê³„ì‚° ê²°ê³¼ ì €ì¥
        with open(checkpoint_path, "wb") as f:
            pickle.dump(shap_values, f)
        logger.info(f"SHAP values saved to {checkpoint_path}.")

    # SHAP ê°’ ì°¨ì› ë³€í™˜ (í•„ìš” ì‹œ)
    if shap_values.shape[-1] == 2:  # (num_samples, num_features, num_classes)
        shap_values = np.transpose(shap_values, (2, 0, 1))  # (num_classes, num_samples, num_features)

    if shap_values.shape[0] != 2:  # í´ë˜ìŠ¤ê°€ 2ê°œê°€ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ
        raise ValueError(f"Unexpected shape for SHAP values after transformation: {np.shape(shap_values)}")

    # SHAP Summary Plot (ì „ì²´ ë°ì´í„°)
    for i, class_shap_values in enumerate(shap_values):
        plt.figure()
        shap.summary_plot(class_shap_values, X_combined, feature_names=feature_names, show=False)
        if i == 0:
            suffix = 'downward'
        else:
            suffix = 'upward'
        plt.title(f"SHAP Summary Plot for Class {i}({suffix})")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"shap_summary_class_{i}({suffix}).png"))
        plt.close()
        logger.info(f"ğŸ“Š SHAP summary plot saved for Class {i}: 'shap_summary_class_{i}.png'")

    # í´ë˜ìŠ¤ë³„ë¡œ ìƒ˜í”Œ 5ê°œì”© ì„ íƒ (í•˜ê°•: 0, ìƒìŠ¹: 1)
    class_0_indices = np.where(y_combined == 0)[0][:100]  # í•˜ê°• ìƒ˜í”Œ 5ê°œ
    class_1_indices = np.where(y_combined == 1)[0][:100]  # ìƒìŠ¹ ìƒ˜í”Œ 5ê°œ

    # explainer ê°’ ì²˜ë¦¬ (SHAP Explainerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
    if explainer is not None:
        expected_value = explainer.expected_value if isinstance(explainer.expected_value, (int, float)) else explainer.expected_value[0]
    else:
        expected_value = 0  # explainerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ 0 ì‚¬ìš©
    
    for sample_idx in class_0_indices:
        shap.force_plot(
            expected_value,
            shap_values[0][sample_idx],
            feature_names=feature_names,
            matplotlib=True
        )
        filename = filenames[sample_idx]
        
        plt.title(f"SHAP Force Plot for {filename} (Downward Intonation)")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"shap_force_downward_{filename}.png"))
        plt.close()
        logger.info(f"ğŸ“Š SHAP force plot saved for {filename} (Downward Intonation)")

    for sample_idx in class_1_indices:
        shap.force_plot(
            expected_value,
            shap_values[1][sample_idx],
            feature_names=feature_names,
            matplotlib=True
        )
        filename = filenames[sample_idx]
        plt.title(f"SHAP Force Plot for {filename} (Upward Intonation)")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"shap_force_upward_{filename}.png"))
        plt.close()
        logger.info(f"ğŸ“Š SHAP force plot saved for {filename} (Upward Intonation)")

    logger.info("SHAP analysis and visualization completed for combined dataset.")
    
# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •: Loss/Accuracy ê¸°ë¡)
def train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=120, checkpoint_path="out/models/best_checkpoint.pth"):
    best_valid_loss = float("inf")
    train_losses, valid_losses = [], []
    train_accuracies, valid_accuracies = [], []

    for epoch in range(num_epochs):
        model.train()
        total_loss, correct = 0, 0
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            correct += (output.argmax(1) == y_batch).sum().item()

        train_acc = correct / len(train_loader.dataset)
        train_losses.append(total_loss / len(train_loader))
        train_accuracies.append(train_acc)
        
        writer.add_scalar("Loss/train", total_loss / len(train_loader), epoch)
        writer.add_scalar("Accuracy/train", train_acc, epoch)
        
        model.eval()
        valid_loss, valid_correct = 0, 0
        with torch.no_grad():
            for X_batch, y_batch in valid_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                valid_loss += loss.item()
                valid_correct += (output.argmax(1) == y_batch).sum().item()
        
        valid_acc = valid_correct / len(valid_loader.dataset)
        valid_losses.append(valid_loss / len(valid_loader))
        valid_accuracies.append(valid_acc)
        writer.add_scalar("Loss/valid", valid_loss / len(valid_loader), epoch)
        writer.add_scalar("Accuracy/valid", valid_acc, epoch)
        
        logger.info(f"Epoch [{epoch+1}/{num_epochs}] Train Loss: {total_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Acc: {valid_acc:.4f}")

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), checkpoint_path)
            logger.info(f"âœ” Best model saved at epoch {epoch+1}")

    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)

# ì„±ëŠ¥ ë¦¬í¬íŠ¸ (Test ê²°ê³¼)
def evaluate_model(model, test_loader, device, dataset):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            output = model(X_batch)
            predictions = output.argmax(1).cpu().numpy()
            y_pred.extend(predictions)
            y_true.extend(y_batch.cpu().numpy())

    # Classification Report
    report = classification_report(y_true, y_pred, target_names=["Falling", "Rising"], output_dict=True)
    logger.info(f"Test Report:\n{pd.DataFrame(report).transpose()}")
    report_path = os.path.join(OUTPUT_DIR, "classification_report.csv")
    pd.DataFrame(report).transpose().to_csv(report_path)
    logger.info(f"ğŸ“Š Classification report saved as '{report_path}'")
    

# ì‹¤í–‰
if __name__ == "__main__":
    train_loader, valid_loader, test_loader, dataset = load_data("training_data.csv")

    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    model = MLP(input_dim=7).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=150)
    evaluate_model(model, test_loader, device, dataset)
    
    
    # ë°ì´í„° ë¡œë“œ (í•™ìŠµ ì—†ì´ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ)
    # train_loader, valid_loader, test_loader, dataset = load_data("training_data.csv")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    # device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

    # ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì´ˆê¸°í™” í•„ìš”)
    # model = MLP(input_dim=7).to(device)

    # ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    checkpoint_path = "out/models/best_checkpoint2.pth"  # ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ
    if os.path.exists(checkpoint_path):
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        logger.info(f"Model checkpoint loaded from '{checkpoint_path}'")
    else:
        raise FileNotFoundError(f"âŒ Error: No checkpoint found at '{checkpoint_path}'")

    # ì‹œê°í™” í•¨ìˆ˜ ì‹¤í–‰ (í•™ìŠµ ì—†ì´ ë°”ë¡œ ë¶„ì„ ì§„í–‰)
    plot_feature_importance(model, dataset)
    plot_shap_analysis_combined(model, valid_loader, test_loader, dataset)

    